{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvcwyBtARcOitaVJfG06BA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradeeplaxkar/Heart-Attack/blob/main/Final_Real_Time_Face_Authorization_(ArcFace_%2B_RetinaFace)_%E2%80%94_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0BdMK9XljGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b73bc47-d8fb-4e2d-957f-330e9acdc564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.3/128.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h25-09-15 07:05:27 - Directory /root/.deepface has been created\n",
            "25-09-15 07:05:27 - Directory /root/.deepface/weights has been created\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# STEP 0: Install deps\n",
        "# ======================\n",
        "!pip install -q opencv-python opencv-contrib-python deepface tensorflow gradio\n",
        "\n",
        "import os, cv2, time, wave, struct, glob\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from deepface import DeepFace\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# STEP 1: Paths & Setup\n",
        "# ======================\n",
        "DB_ROOT = \"authorized_faces\"   # <-- Put one subfolder per person: authorized_faces/Alice/*.jpg\n",
        "os.makedirs(DB_ROOT, exist_ok=True)\n",
        "\n",
        "# Helpful: show instructions\n",
        "print(\"\"\"\n",
        "Upload enrollment images like:\n",
        "\n",
        "authorized_faces/\n",
        "  Alice/\n",
        "    1.jpg\n",
        "    2.jpg\n",
        "  Bob/\n",
        "    selfie1.png\n",
        "    office2.jpg\n",
        "\n",
        "Then run the next cell to build the database.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBzAFcN6lsJK",
        "outputId": "10bc6d84-37b6-4741-bbb9-58eebb599b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Upload enrollment images like:\n",
            "\n",
            "authorized_faces/\n",
            "  Alice/\n",
            "    1.jpg\n",
            "    2.jpg\n",
            "  Bob/\n",
            "    selfie1.png\n",
            "    office2.jpg\n",
            "\n",
            "Then run the next cell to build the database.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# STEP 2: Utilities\n",
        "# ======================\n",
        "def l2_normalize(v):\n",
        "    v = np.asarray(v, dtype=np.float32)\n",
        "    return v / (norm(v) + 1e-12)\n",
        "\n",
        "def euclidean(a, b):\n",
        "    return norm(a - b)\n",
        "\n",
        "def variance_of_laplacian(img_gray):\n",
        "    # focus measure (higher = sharper)\n",
        "    return cv2.Laplacian(img_gray, cv2.CV_64F).var()\n",
        "\n",
        "def mean_brightness(img_gray):\n",
        "    return float(np.mean(img_gray))\n",
        "\n",
        "def ensure_uint8(img):\n",
        "    \"\"\"DeepFace.extract_faces returns aligned 'face' in [0,1] float.\n",
        "       Convert safely to uint8 if needed.\"\"\"\n",
        "    arr = img\n",
        "    if arr.dtype != np.uint8:\n",
        "        arr = np.clip(arr, 0.0, 1.0) * 255.0\n",
        "        arr = arr.astype(\"uint8\")\n",
        "    return arr\n"
      ],
      "metadata": {
        "id": "12KETMijlye7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# STEP 3: Build Database\n",
        "# ======================\n",
        "# We compute embeddings with:\n",
        "#   - Detection/Alignment: RetinaFace (only at enrollment)\n",
        "#   - Embedding model: ArcFace\n",
        "# We then compute per-person centroid and adaptive threshold.\n",
        "\n",
        "EMBED_MODEL = \"ArcFace\"      # SOTA\n",
        "DET_BACKEND = \"retinaface\"   # robust detector\n",
        "\n",
        "class PersonEntry:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.embeddings = []   # list of l2-normalized embeddings\n",
        "        self.centroid = None\n",
        "        self.threshold = 0.60  # fallback; will be adapted\n",
        "\n",
        "    def finalize(self):\n",
        "        if len(self.embeddings) == 0:\n",
        "            return\n",
        "        # centroid of normalized embeddings, then re-normalize\n",
        "        C = np.mean(np.stack(self.embeddings, axis=0), axis=0)\n",
        "        self.centroid = l2_normalize(C)\n",
        "\n",
        "        # adaptive threshold: 90th percentile of in-class distances + margin\n",
        "        dists = [euclidean(e, self.centroid) for e in self.embeddings]\n",
        "        if len(dists) >= 3:\n",
        "            p90 = float(np.percentile(dists, 90))\n",
        "            self.threshold = max(0.35, min(0.80, p90 + 0.05))  # clamp to sensible range\n",
        "        elif len(dists) == 2:\n",
        "            self.threshold = max(0.45, min(0.75, np.mean(dists) + 0.05))\n",
        "        else:\n",
        "            # single image: keep conservative default\n",
        "            self.threshold = 0.60\n",
        "\n",
        "authorized_db = []  # list[PersonEntry]\n",
        "\n",
        "def build_database(db_root=DB_ROOT):\n",
        "    authorized_db.clear()\n",
        "\n",
        "    # detect if user used subfolders (recommended)\n",
        "    subfolders = [p for p in glob.glob(os.path.join(db_root, \"*\")) if os.path.isdir(p)]\n",
        "    used_subfolders = len(subfolders) > 0\n",
        "\n",
        "    if not used_subfolders:\n",
        "        print(\"⚠ No subfolders found. Using each image in root as a separate identity by its filename stem.\")\n",
        "        image_paths = [p for p in glob.glob(os.path.join(db_root, \"*\")) if os.path.isfile(p)]\n",
        "        # group by stem (before dot)\n",
        "        groups = {}\n",
        "        for p in image_paths:\n",
        "            name = os.path.splitext(os.path.basename(p))[0]\n",
        "            groups.setdefault(name, []).append(p)\n",
        "        name_to_images = groups.items()\n",
        "    else:\n",
        "        name_to_images = []\n",
        "        for person_dir in subfolders:\n",
        "            person = os.path.basename(person_dir)\n",
        "            imgs = [p for p in glob.glob(os.path.join(person_dir, \"*\")) if os.path.isfile(p)]\n",
        "            if imgs:\n",
        "                name_to_images.append((person, imgs))\n",
        "\n",
        "    n_people = 0\n",
        "    for name, imgs in name_to_images:\n",
        "        entry = PersonEntry(name)\n",
        "        for img_path in imgs:\n",
        "            try:\n",
        "                # represent does detection+alignment here ONCE at enrollment\n",
        "                rep = DeepFace.represent(\n",
        "                    img_path,\n",
        "                    model_name=EMBED_MODEL,\n",
        "                    detector_backend=DET_BACKEND,\n",
        "                    enforce_detection=True\n",
        "                )\n",
        "                # DeepFace.represent returns a list of detections\n",
        "                for item in rep:\n",
        "                    emb = l2_normalize(np.array(item[\"embedding\"], dtype=np.float32))\n",
        "                    entry.embeddings.append(emb)\n",
        "                print(f\"✅ {name}: +{len(rep)} face(s) from {os.path.basename(img_path)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ {name} - {os.path.basename(img_path)}: {e}\")\n",
        "\n",
        "        if len(entry.embeddings) > 0:\n",
        "            entry.finalize()\n",
        "            print(f\"   ↳ centroid set; adaptive threshold={entry.threshold:.3f}; samples={len(entry.embeddings)}\")\n",
        "            authorized_db.append(entry)\n",
        "            n_people += 1\n",
        "\n",
        "    print(f\"\\n📚 Database built: {n_people} identities.\")\n",
        "    if n_people == 0:\n",
        "        print(\"➡️ Add images to authorized_faces/<NAME>/ and run this cell again.\")\n",
        "\n",
        "# Run this after uploading your images\n",
        "build_database()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bka3vjaYl1CN",
        "outputId": "22c66d60-2d49-4970-d466-814009d94d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25-09-15 07:09:19 - 🔗 arcface_weights.h5 will be downloaded from https://github.com/serengil/deepface_models/releases/download/v1.0/arcface_weights.h5 to /root/.deepface/weights/arcface_weights.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/arcface_weights.h5\n",
            "To: /root/.deepface/weights/arcface_weights.h5\n",
            "100%|██████████| 137M/137M [00:00<00:00, 207MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25-09-15 07:09:23 - retinaface.h5 will be downloaded from the url https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n",
            "To: /root/.deepface/weights/retinaface.h5\n",
            "100%|██████████| 119M/119M [00:00<00:00, 240MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_03_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_06_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_09_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_07_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_00_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_06_59_Pro (2).jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_08_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_04_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_06_55_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_05_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_07_01_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_06_57_Pro.jpg\n",
            "✅ Pradeep: +2 face(s) from WIN_20250914_14_07_02_Pro.jpg\n",
            "✅ Pradeep: +1 face(s) from WIN_20250914_14_06_59_Pro.jpg\n",
            "   ↳ centroid set; adaptive threshold=0.800; samples=15\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_07_Pro.jpg\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_04_Pro (2).jpg\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_02_Pro.jpg\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_03_Pro (2).jpg\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_06_Pro (2).jpg\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_03_Pro.jpg\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_06_Pro.jpg\n",
            "✅ Registrar: +1 face(s) from WIN_20250915_10_25_04_Pro.jpg\n",
            "   ↳ centroid set; adaptive threshold=0.800; samples=8\n",
            "✅ Provost Sir: +1 face(s) from WIN_20250915_10_23_05_Pro (2).jpg\n",
            "✅ Provost Sir: +1 face(s) from WIN_20250915_10_23_09_Pro.jpg\n",
            "✅ Provost Sir: +1 face(s) from WIN_20250915_10_23_05_Pro.jpg\n",
            "✅ Provost Sir: +1 face(s) from WIN_20250915_10_23_08_Pro.jpg\n",
            "✅ Provost Sir: +1 face(s) from WIN_20250915_10_23_06_Pro.jpg\n",
            "✅ Provost Sir: +1 face(s) from WIN_20250915_10_23_07_Pro.jpg\n",
            "✅ Provost Sir: +1 face(s) from WIN_20250915_10_23_08_Pro (2).jpg\n",
            "   ↳ centroid set; adaptive threshold=0.647; samples=7\n",
            "\n",
            "📚 Database built: 3 identities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# STEP 5: Live Inference\n",
        "# ======================\n",
        "# Important: we use DeepFace.extract_faces to get ALIGNED chips: f[\"face\"]\n",
        "# Then we embed with detector_backend=\"skip\" to avoid re-detection.\n",
        "# Add quality gates + temporal smoothing\n",
        "\n",
        "MIN_FACE = 60            # min height/width of face chip in px\n",
        "MIN_BRIGHT = 20          # mean grayscale lower bound\n",
        "MAX_BRIGHT = 235         # mean grayscale upper bound\n",
        "MIN_SHARPNESS = 20.0     # variance of Laplacian threshold\n",
        "VOTE_N = 3               # require 3 consecutive \"authorized\" frames per face to confirm\n",
        "FRAME_COOLDOWN = 0.75    # seconds between beeps\n",
        "\n",
        "last_beep_ts = 0.0\n",
        "consec_auth = 0  # simple stream-wide vote; good enough for single-subject demos\n",
        "\n",
        "def classify_embedding(emb):\n",
        "    \"\"\"Return (is_authorized, best_name, best_dist, best_thresh) using per-person centroid+threshold.\"\"\"\n",
        "    if not authorized_db:\n",
        "        return False, None, 9e9, 0.0\n",
        "    emb = l2_normalize(np.array(emb, dtype=np.float32))\n",
        "    best_name, best_dist, best_thresh = None, 9e9, 0.0\n",
        "    for person in authorized_db:\n",
        "        d = euclidean(emb, person.centroid)\n",
        "        if d < best_dist:\n",
        "            best_dist = d\n",
        "            best_name = person.name\n",
        "            best_thresh = person.threshold\n",
        "    return (best_dist < best_thresh), best_name, float(best_dist), float(best_thresh)\n",
        "\n",
        "def live_check(frame_rgb):\n",
        "    global last_beep_ts, consec_auth\n",
        "\n",
        "    # Convert to BGR for OpenCV ops\n",
        "    frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    try:\n",
        "        detections = DeepFace.extract_faces(\n",
        "            frame_bgr,\n",
        "            detector_backend=DET_BACKEND,\n",
        "            enforce_detection=False\n",
        "        )\n",
        "    except Exception:\n",
        "        detections = []\n",
        "\n",
        "    any_unauth = False\n",
        "    any_face = False\n",
        "\n",
        "    for f in detections or []:\n",
        "        fa = f.get(\"facial_area\", {})\n",
        "        x, y, w, h = int(fa.get(\"x\", 0)), int(fa.get(\"y\", 0)), int(fa.get(\"w\", 0)), int(fa.get(\"h\", 0))\n",
        "        x2, y2 = x + w, y + h\n",
        "        if w <= 0 or h <= 0:\n",
        "            continue\n",
        "\n",
        "        # Use ALIGNED face chip from DeepFace\n",
        "        chip = f.get(\"face\", None)\n",
        "        if chip is None:\n",
        "            # fallback to raw crop (less reliable)\n",
        "            chip = frame_bgr[max(0,y):max(0,y2), max(0,x):max(0,x2)]\n",
        "        chip = ensure_uint8(chip)\n",
        "\n",
        "        # Quality gates\n",
        "        #if chip.shape[0] < MIN_FACE or chip.shape[1] < MIN_FACE:\n",
        "            #label = \"Face too small — move closer\"\n",
        "            #color = (255, 165, 0)  # orange\n",
        "            #cv2.rectangle(frame_bgr, (x, y), (x2, y2), color, 2)\n",
        "            #cv2.putText(frame_bgr, label, (x, max(0, y-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            #continue\n",
        "\n",
        "        gray = cv2.cvtColor(chip, cv2.COLOR_BGR2GRAY)\n",
        "        bright = mean_brightness(gray)\n",
        "        sharp = variance_of_laplacian(gray)\n",
        "        #if not (MIN_BRIGHT <= bright <= MAX_BRIGHT) or sharp < MIN_SHARPNESS:\n",
        "            #label = \"Low quality — add light / hold steady\"\n",
        "            #color = (255, 165, 0)\n",
        "            #cv2.rectangle(frame_bgr, (x, y), (x2, y2), color, 2)\n",
        "            #cv2.putText(frame_bgr, label, (x, max(0, y-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            #continue\n",
        "\n",
        "        any_face = True\n",
        "\n",
        "        # Embed WITHOUT re-detecting: detector_backend=\"skip\"\n",
        "        try:\n",
        "            rep = DeepFace.represent(\n",
        "                chip,\n",
        "                model_name=EMBED_MODEL,\n",
        "                detector_backend=\"skip\",   # <— critical\n",
        "                enforce_detection=False\n",
        "            )\n",
        "            if not rep or not isinstance(rep, list):\n",
        "                continue\n",
        "            emb = rep[0][\"embedding\"]\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        ok, name, dist, thr = classify_embedding(emb)\n",
        "\n",
        "        if ok:\n",
        "            consec_auth = min(VOTE_N, consec_auth + 1)\n",
        "            confirmed = (consec_auth >= VOTE_N)\n",
        "            color = (0, 200, 0) if confirmed else (0, 180, 120)\n",
        "            label = f\"Authorized: {name}  d={dist:.2f} < {thr:.2f}\" if confirmed else f\"Checking… {name} d={dist:.2f}\"\n",
        "        else:\n",
        "            consec_auth = 0\n",
        "            any_unauth = True\n",
        "            color = (0, 0, 255)\n",
        "            label = f\"Unauthorized  d={dist:.2f} ≥ {thr:.2f}\"\n",
        "\n",
        "        cv2.rectangle(frame_bgr, (x, y), (x2, y2), color, 2)\n",
        "        cv2.putText(frame_bgr, label, (x, max(0, y-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "\n",
        "    # Decide beep + status\n",
        "    if any_face and not any_unauth and consec_auth >= VOTE_N:\n",
        "        status = \"✅ Authorized\"\n",
        "        beep = None\n",
        "    elif any_unauth:\n",
        "        status = \"⚠ Unauthorized detected!\"\n",
        "        # throttle beep a little to avoid constant noise\n",
        "        now = time.time()\n",
        "        beep = \"alert.wav\" if (now - last_beep_ts) >= FRAME_COOLDOWN else None\n",
        "        if beep: last_beep_ts = now\n",
        "    else:\n",
        "        status = \"🙂 Show your face to the camera\"\n",
        "        beep = None\n",
        "        consec_auth = 0\n",
        "\n",
        "    return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB), beep, status\n"
      ],
      "metadata": {
        "id": "UOpcr6Z3mmaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# STEP 6: Gradio UI\n",
        "# ======================\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 👁️ Real-Time Face Authorization (ArcFace + RetinaFace) — Colab\\n\"\n",
        "                \"Aligned chips, adaptive thresholds, quality gates, temporal smoothing.\")\n",
        "    with gr.Row():\n",
        "        cam = gr.Image(sources=\"webcam\", streaming=True, type=\"numpy\", label=\"Webcam\")\n",
        "        out = gr.Image(type=\"numpy\", label=\"Detections\")\n",
        "    beep_audio = gr.Audio(label=\"Beep\", autoplay=True)\n",
        "    status = gr.Label(label=\"Status\")\n",
        "    cam.stream(live_check, inputs=cam, outputs=[out, beep_audio, status])\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "tQPoglnxmuN1",
        "outputId": "a4b11974-1a31-4a5e-e63a-018ce21ea429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7480ede62062480ef6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7480ede62062480ef6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7YiUVKc8hXX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}